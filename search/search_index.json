{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#usage","title":"Usage","text":"<ul> <li>Basic Usage: Use <code>product_similarity_main.ipynb</code></li> <li> <p>Configuration:</p> </li> <li> <p>Create new enviroment, due to hatch not been able to handle GPU resources   <pre><code>conda create --name gpu_torch python&gt;3.9 --y\nconda activate gpu_torch\npip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\npip install -r requirements_torch.txt\n</code></pre></p> </li> <li>Modify pyproject.toml to add or remove packages to run CPU packages and manager local packages</li> </ul>"},{"location":"index.html#data","title":"Data","text":"<ul> <li>Sources: The dataset is collected by CPG distributor public site.</li> <li>Structure: Table of key features</li> </ul> <p>Example</p> <p>Input data format</p> Text <code>string</code>"},{"location":"index.html#result","title":"Result \u2705","text":"<ul> <li>Findings:</li> <li>After fine tuning the model, the results are the top 2 products that are similar to the given product based on their product descriptions.</li> <li> <p>Overall, these can be used to find comepetitors in the market or complemetary products  as transitions can be seen. As well to aid other possible niches in the market product segment to innovate.</p> </li> <li> <p>Visualizations:</p> </li> <li>Example visualizations (if applicable). </li> </ul>"},{"location":"index.html#directory-structure","title":"Directory Structure","text":"<pre><code>.\n\u251c\u2500\u2500 docs &lt;- markdown files for mkdocs\n\u2502   \u2514\u2500\u2500 img &lt;- assets\n\u251c\u2500\u2500 notebooks &lt;- jupyter notebooks for exploratory analysis and explanation\n\u2514\u2500\u2500 src - scripts for processing data eg. transformations, dataset merges etc.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data &lt;- loading, saving and modelling your data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 features &lt;- feature engineering\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model &lt;- algorithms and models\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 plots &lt;- plots\n\u2502   \u2514\u2500\u2500 utils &lt;- api and other\n\u251c\u2500\u2500 LICENSE &lt;- License\n\u251c\u2500\u2500 mkdocs.yml &lt;- config for mkdocs\n\u251c\u2500\u2500 pyproject.yml &lt;- config project\n\u2514\u2500\u2500 README.md &lt;- README file of the package\n</code></pre>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>To contribute create a PR a use conventional commits</p> <pre><code>fix: &lt;description&gt;\nfeat: &lt;description&gt;\ndocs: &lt;description&gt;\nrefactor: &lt;description&gt;\n</code></pre> <p>License</p> <p>The project is licensed under the MIT License.</p> <p>I hope this is helpful!</p>"},{"location":"data.html","title":"Data","text":""},{"location":"data.html#data.etl","title":"<code>data.etl</code>","text":"<p>General ETL process to move from interm to processed file add data to deployed stage</p>"},{"location":"data.html#data.etl.TextCleaner","title":"<code>TextCleaner</code>","text":"<p>Clean text data by removing stopwords, punctuation, new spaces / tabs and converting to lowercase.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>cleaned text</p> Source code in <code>src/data/etl.py</code> <pre><code>class TextCleaner:\n    \"\"\"Clean text data by removing stopwords, punctuation, new spaces / tabs and converting to lowercase.\n\n    Returns:\n        str: cleaned text\n    \"\"\"\n\n    import string\n    from typing import List  # noqa: UP035\n\n    def __init__(self, additional_stopwords: List[str] = None):  # noqa: RUF013, UP006\n        from nltk.corpus import stopwords\n\n        \"\"\"Initialize the TextCleaner with optional additional stopwords.\n        \"\"\"\n        # Default stopwords from NLTK\n        self.stopwords = set(stopwords.words(\"english\"))\n        if additional_stopwords:\n            self.stopwords.update(word.lower() for word in additional_stopwords)\n\n    def remove_stopwords(self, text: str) -&gt; str:\n        \"\"\"removes stopwords from a string\n\n        Args:\n            text (str): text with stopwords\n\n        Returns:\n            str: text without stopwords\n        \"\"\"\n        words = text.split()\n        filtered_words = [word for word in words if word.lower() not in self.stopwords]\n        return \" \".join(filtered_words)\n\n    def remove_punctuation(self, text: str, punct: str = string.punctuation) -&gt; str:\n        \"\"\"Remove punctuation from the text.\n\n        Args:\n            text (str): text with punctuation\n            punct (str, optional): Punctuation to remove. Defaults to string.punctuation.\n\n        Returns:\n            str: text without punctuation\n        \"\"\"\n        return \"\".join(char for char in text if char not in punct)\n\n    def unicode(self, text: str) -&gt; str:\n        import unidecode\n\n        \"\"\"converts unicode characters to ASCII\n\n        Returns:\n            _type_: converted text\n        \"\"\"\n        return unidecode.unidecode(text)\n\n    def remove_newline_tabs_spaces(self, text: str) -&gt; str:\n        \"\"\"Removes newlines and tabs from a string and replaces them with spaces\n\n        Args:\n            text (str): text with newlines and tabs\n\n        Returns:\n            str: cleaned text\n        \"\"\"\n        # Replace newlines and tabs with spaces\n        text = re.sub(r\"[\\n\\t]+\", \" \", text)\n        # Optionally remove extra spaces\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n\n    def clean(self, text: str) -&gt; str:\n        \"\"\"Apply all cleaning steps to the text.\n\n        Args:\n            text (str): unpocessed text\n\n        Returns:\n            str: processed text\n        \"\"\"\n        text = self.unicode(text)\n        text = self.remove_punctuation(text)\n        text = self.remove_stopwords(text)\n        text = self.remove_newline_tabs_spaces(text)\n        return text.lower()\n</code></pre>"},{"location":"data.html#data.etl.TextCleaner.clean","title":"<code>clean(text)</code>","text":"<p>Apply all cleaning steps to the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>unpocessed text</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>processed text</p> Source code in <code>src/data/etl.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Apply all cleaning steps to the text.\n\n    Args:\n        text (str): unpocessed text\n\n    Returns:\n        str: processed text\n    \"\"\"\n    text = self.unicode(text)\n    text = self.remove_punctuation(text)\n    text = self.remove_stopwords(text)\n    text = self.remove_newline_tabs_spaces(text)\n    return text.lower()\n</code></pre>"},{"location":"data.html#data.etl.TextCleaner.remove_newline_tabs_spaces","title":"<code>remove_newline_tabs_spaces(text)</code>","text":"<p>Removes newlines and tabs from a string and replaces them with spaces</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text with newlines and tabs</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>cleaned text</p> Source code in <code>src/data/etl.py</code> <pre><code>def remove_newline_tabs_spaces(self, text: str) -&gt; str:\n    \"\"\"Removes newlines and tabs from a string and replaces them with spaces\n\n    Args:\n        text (str): text with newlines and tabs\n\n    Returns:\n        str: cleaned text\n    \"\"\"\n    # Replace newlines and tabs with spaces\n    text = re.sub(r\"[\\n\\t]+\", \" \", text)\n    # Optionally remove extra spaces\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n</code></pre>"},{"location":"data.html#data.etl.TextCleaner.remove_punctuation","title":"<code>remove_punctuation(text, punct=string.punctuation)</code>","text":"<p>Remove punctuation from the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text with punctuation</p> required <code>punct</code> <code>str</code> <p>Punctuation to remove. Defaults to string.punctuation.</p> <code>punctuation</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>text without punctuation</p> Source code in <code>src/data/etl.py</code> <pre><code>def remove_punctuation(self, text: str, punct: str = string.punctuation) -&gt; str:\n    \"\"\"Remove punctuation from the text.\n\n    Args:\n        text (str): text with punctuation\n        punct (str, optional): Punctuation to remove. Defaults to string.punctuation.\n\n    Returns:\n        str: text without punctuation\n    \"\"\"\n    return \"\".join(char for char in text if char not in punct)\n</code></pre>"},{"location":"data.html#data.etl.TextCleaner.remove_stopwords","title":"<code>remove_stopwords(text)</code>","text":"<p>removes stopwords from a string</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text with stopwords</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>text without stopwords</p> Source code in <code>src/data/etl.py</code> <pre><code>def remove_stopwords(self, text: str) -&gt; str:\n    \"\"\"removes stopwords from a string\n\n    Args:\n        text (str): text with stopwords\n\n    Returns:\n        str: text without stopwords\n    \"\"\"\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in self.stopwords]\n    return \" \".join(filtered_words)\n</code></pre>"},{"location":"data.html#data.etl.apply_function_to_non_integer_columns","title":"<code>apply_function_to_non_integer_columns(df, func)</code>","text":"<p>Applies the given function to each column in the DataFrame that is object type dtype. Used for cleaning up text data in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to process.</p> required <code>func</code> <code>callable</code> <p>The function to apply to each non-integer column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with non-integer columns processed by the given function.</p> Source code in <code>src/data/etl.py</code> <pre><code>def apply_function_to_non_integer_columns(df: pd.DataFrame, func) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies the given function to each column in the DataFrame that is object type dtype.\n    Used for cleaning up text data in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to process.\n        func (callable): The function to apply to each non-integer column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with non-integer columns processed by the given function.\n    \"\"\"\n    for col in df.columns:\n        if df[col].dtype == \"object\":  # Check if column contains non-integer data\n            print(f\"Processing column: {col}\")\n            df[col] = df[col].apply(func)\n    return df\n</code></pre>"},{"location":"data.html#data.etl.backup_file","title":"<code>backup_file(path_csv_deployed, dst)</code>","text":"<p>copies file for archives</p> <p>Parameters:</p> Name Type Description Default <code>path_csv_deployed</code> <code>str</code> <p>path of file to back up</p> required <code>dst</code> <code>str</code> <p>path destination of file to save to</p> required Source code in <code>src/data/etl.py</code> <pre><code>def backup_file(path_csv_deployed: str, dst: str) -&gt; None:\n    \"\"\"copies file for archives\n\n    Args:\n        path_csv_deployed (str): path of file to back up\n        dst (str): path destination of file to save to\n    \"\"\"\n    import shutil\n\n    shutil.copy(path_csv_deployed, dst)\n</code></pre>"},{"location":"data.html#data.etl.csv_combine_proc","title":"<code>csv_combine_proc(paths)</code>","text":"<p>combines all datasets from the interim stage</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> <p>paths from interim datasets</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: combined dataframe</p> Source code in <code>src/data/etl.py</code> <pre><code>def csv_combine_proc(paths: list) -&gt; pd.DataFrame:\n    \"\"\"combines all datasets from the interim stage\n\n    Args:\n        paths (list): paths from interim datasets\n\n    Returns:\n        pd.DataFrame: combined dataframe\n    \"\"\"\n    import datetime\n\n    import pandas as pd\n\n    df = pd.DataFrame()\n    for file in paths:\n        filename = file.split(\"\\\\\")[8].split(\".\")[0]\n        print(\"Folder - \" + filename)\n\n        try:\n            df_temp = pd.read_csv(file)\n            df_temp[\"Source.Name.Interim\"] = filename\n\n            now = datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%d\")\n            # date ran\n            df_temp[\"proccessed\"] = now\n            df = pd.concat([df, df_temp], axis=0)\n\n        except pd.errors.EmptyDataError:\n            print(\"Folder \" + filename + \" is blank. Skipping file.\")\n    return df\n</code></pre>"},{"location":"data.html#data.etl.csv_combine_update_dep","title":"<code>csv_combine_update_dep(paths, path_csv_deployed, ref_col)</code>","text":"<p>combines datasets from deployed and processed stage removing     duplicated files from deployed stage if processed file     has same file name (considers for updated data in new files).     CONFIRM file names are the SAME if not it will     duplicate data.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> <p>paths from processed datasets</p> required <code>path_csv_deployed</code> <code>str</code> <p>path of deployed dataset</p> required <code>ref_col</code> <code>str</code> <p>reference column to avoid duplicated dated</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: combined dataset from processed and existing deployed</p> Source code in <code>src/data/etl.py</code> <pre><code>def csv_combine_update_dep(paths: list, path_csv_deployed: str, ref_col: str) -&gt; pd.DataFrame:\n    \"\"\"combines datasets from deployed and processed stage removing\n        duplicated files from deployed stage if processed file\n        has same file name (considers for updated data in new files).\n        CONFIRM file names are the SAME if not it will\n        duplicate data.\n\n    Args:\n        paths (list): paths from processed datasets\n        path_csv_deployed (str): path of deployed dataset\n        ref_col (str): reference column to avoid duplicated dated\n\n    Returns:\n        pd.DataFrame: combined dataset from processed and existing deployed\n    \"\"\"\n    import datetime\n\n    import pandas as pd\n\n    df_deployed = pd.read_csv(path_csv_deployed)\n\n    for file in paths:\n        filename = file.split(\"\\\\\")[8]\n        print(filename)\n\n        df_temp = pd.read_csv(file)\n\n        # date ran\n        now = datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%d\")\n        df_temp[\"deployed\"] = now\n\n        # v2\n        # removes files with the same file path in deployed\n        # if it reuploads it keeps one file (help with updates and duplicated files)\n        filenames = df_deployed[ref_col]\n\n        # unique set of deployed file names\n        filenames = set(filenames)\n\n        filenames_temp = df_temp[ref_col]\n\n        # unique set of processed file names\n        filenames_temp = set(filenames_temp)\n        # find matching names\n        updated = filenames.intersection(filenames_temp)\n        print(\"Updating ...\")\n        print(updated)\n        # remove matching file names based on the ref_col\n        df_deployed = df_deployed.loc[~df_deployed[ref_col].isin(updated)]\n\n        # combine datasets\n        df_deployed = pd.concat([df_deployed, df_temp], axis=0)\n\n    return df_deployed\n</code></pre>"},{"location":"data.html#data.etl.csv_dep_init","title":"<code>csv_dep_init(paths)</code>","text":"<p>Initilizes dataset to next stage to deployment from proccessed</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> <p>paths from processed datasets</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: dataset from proccessed initialized</p> Source code in <code>src/data/etl.py</code> <pre><code>def csv_dep_init(paths: list) -&gt; pd.DataFrame:\n    \"\"\"Initilizes dataset to next stage to deployment from proccessed\n\n    Args:\n        paths (list): paths from processed datasets\n\n    Returns:\n        pd.DataFrame: dataset from proccessed initialized\n    \"\"\"\n    import datetime\n\n    import pandas as pd\n\n    for file in paths:\n        filename = file.split(\"\\\\\")[8]\n        print(filename)\n\n        df_temp = pd.read_csv(file)\n\n        # date ran\n        now = datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%d\")\n        df_temp[\"deployed\"] = now\n\n    return df_temp\n</code></pre>"},{"location":"data.html#data.etl.datafile_path_finder","title":"<code>datafile_path_finder(file_name)</code>","text":"<p>Constructs a path by combining the parent directory of the current working directory with the 'data' folder and the provided file name. If no file name is provided, a default path is returned.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file for which the path is to be determined.</p> required <p>Returns:</p> Name Type Description <code>df_dir</code> <code>str</code> <p>The full path to the file, or an indication if no file name was provided.</p> Source code in <code>src/data/etl.py</code> <pre><code>def datafile_path_finder(file_name: str) -&gt; str:\n    \"\"\"\n    Constructs a path by combining the parent directory of the current working directory with the 'data' folder\n    and the provided file name. If no file name is provided, a default path is returned.\n\n    Args:\n        file_name (str): The name of the file for which the path is to be determined.\n\n    Returns:\n        df_dir (str): The full path to the file, or an indication if no file name was provided.\n    \"\"\"\n    import glob\n    import os\n\n    main_dir = os.path.dirname(os.getcwd())\n    rawdata_dir = os.path.join(main_dir, \"data\", file_name)\n    df_dir = glob.glob(rawdata_dir)[0]\n    return df_dir\n</code></pre>"},{"location":"data.html#data.etl.find_nan","title":"<code>find_nan(df)</code>","text":"<p>finds all NaN values in a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to search for NaN values</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: count of NaN values in each column</p> Source code in <code>src/data/etl.py</code> <pre><code>def find_nan(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"finds all NaN values in a dataframe\n\n    Args:\n        df (pd.DataFrame): dataframe to search for NaN values\n\n    Returns:\n        pd.DataFrame: count of NaN values in each column\n    \"\"\"\n\n    return df.isnull().sum()\n</code></pre>"},{"location":"model.html","title":"Models","text":""},{"location":"model.html#model.transformers","title":"<code>model.transformers</code>","text":""},{"location":"model.html#model.transformers.get_embedding","title":"<code>get_embedding(text, tokenizer, model, device)</code>","text":"<p>Get the mean embedding of a text using a transformer model</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text to embed</p> required <code>tokenizer</code> <code>dict</code> <p>tokenizer object</p> required <code>model</code> <code>str</code> <p>model object</p> required <code>device</code> <code>str</code> <p>device to use</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: mean embedding of the text</p> Source code in <code>src/model/transformers.py</code> <pre><code>def get_embedding(text: str, tokenizer: dict, model: str, device: str) -&gt; np.ndarray:\n    \"\"\"Get the mean embedding of a text using a transformer model\n\n    Args:\n        text (str): text to embed\n        tokenizer (dict): tokenizer object\n        model (str): model object\n        device (str): device to use\n\n    Returns:\n        np.ndarray: mean embedding of the text\n    \"\"\"\n    import torch\n\n    # Tokenize and move to device\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    with torch.no_grad():\n        # Forward pass to get model outputs\n        outputs = model(**inputs)\n\n        # Access last_hidden_state\n        last_hidden_state = outputs.last_hidden_state\n        # Compute mean of last hidden state and convert to numpy array\n        mean_embedding = last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n\n    return mean_embedding\n</code></pre>"}]}